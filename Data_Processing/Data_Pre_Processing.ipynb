{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Twibot-20 Dataset\n",
    "Following the pre-processing steps as mentioned on \"Deep Neural Networks for Bot Detection\" paper."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bfe8b8ffbf169fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29981be726302966"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ce9432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:15:06.921047300Z",
     "start_time": "2025-03-29T18:15:06.912943Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Torch \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Reading Data\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5be1bc2e756e4b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11b337e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T17:59:14.793217Z",
     "start_time": "2025-03-29T17:59:13.742557900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\djsal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\djsal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8134a2",
   "metadata": {},
   "source": [
    "## Pre Processing Data for each set (test, train, validate) and returning metadata tensor, tweet glove embeddings tensor, and labels tensor for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6bddeac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:02:56.097958900Z",
     "start_time": "2025-03-29T18:02:56.091804300Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file_path):\n",
    "        embeddings = {}\n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "        return embeddings\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def embed_text(text, glove_embeddings):\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        embedding = []\n",
    "        \n",
    "        # Get GloVe embeddings for each word in the tokenized text\n",
    "        for word in tokens:\n",
    "            \n",
    "            # Applying Kudugunta's Rules\n",
    "            if word == '#':\n",
    "                \n",
    "                word = \"<hashtag>\"\n",
    "            \n",
    "            elif word == '@':\n",
    "                \n",
    "                word = \"<user>\"\n",
    "            \n",
    "            elif word == \"https\" or word == \"HTTPS\":\n",
    "                \n",
    "                word = \"<url>\"\n",
    "            \n",
    "            elif word[0:3] == \"//t\":\n",
    "                \n",
    "                word = \"<url>\"\n",
    "            \n",
    "            elif word.isdigit() or is_float(word):\n",
    "                \n",
    "                word = \"<number>\"\n",
    "                \n",
    "            # Replacing emojis\n",
    "\n",
    "            if emoji.is_emoji(word):\n",
    "                word = emoji.demojize(word)\n",
    "                word = \"<\" + word[1:-1] + \">\"\n",
    "\n",
    "                            \n",
    "            # For word in all caps\n",
    "            if word.isupper():\n",
    "                \n",
    "                first_word = word.lower()\n",
    "                \n",
    "                if first_word in glove_embeddings:\n",
    "                    embedding.append(glove_embeddings[first_word])\n",
    "                else:\n",
    "                    # If the word is not in GloVe, append a zero vector (or you can handle it differently)\n",
    "                    embedding.append(np.zeros(200))  # Assuming the GloVe embeddings are 200-dimensional\n",
    "                    \n",
    "                second_word = \"<allcaps>\"\n",
    "                \n",
    "                if second_word in glove_embeddings:\n",
    "                    \n",
    "                    embedding.append(glove_embeddings[second_word])\n",
    "                else:\n",
    "                    # If the word is not in GloVe, append a zero vector (or you can handle it differently)\n",
    "                    embedding.append(np.zeros(200))  # Assuming the GloVe embeddings are 200-dimensional\n",
    "            \n",
    "            else: \n",
    "                word = word.lower()\n",
    "                \n",
    "                if word in glove_embeddings:\n",
    "                    embedding.append(glove_embeddings[word])\n",
    "                else:\n",
    "                    # If the word is not in GloVe, append a zero vector (or you can handle it differently)\n",
    "                    embedding.append(np.zeros(200))  # Assuming the GloVe embeddings are 200-dimensional\n",
    "\n",
    "        # Return the average embedding for the entire sentence (or you can return a list of vectors)\n",
    "        if embedding:\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(200)  # Default if no tokens are found in the embeddings\n",
    "        \n",
    "def encode_verified(row):\n",
    "    \n",
    "    row = row.strip()\n",
    "    if row == \"False\":\n",
    "        \n",
    "        return  0\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c101693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:03:00.316083800Z",
     "start_time": "2025-03-29T18:03:00.311565200Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(data_file_path):\n",
    "    \n",
    "    # Reading from json file\n",
    "    # Open and read the JSON file\n",
    "    with open(data_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Flatten the JSON using json_normalize\n",
    "    flattened_data = pd.json_normalize(data)\n",
    "\n",
    "    # Convert the flattened data into a Pandas DataFrame\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    \n",
    "    # Getting relevant columns\n",
    "    numerical_cols = ['profile.followers_count', 'profile.friends_count',\n",
    "                  'profile.favourites_count', 'profile.listed_count']\n",
    "\n",
    "    categorical_cols = ['profile.verified']\n",
    "\n",
    "    text_cols = ['tweet']\n",
    "\n",
    "    labels = ['label']\n",
    "\n",
    "    relevant_cols = numerical_cols + categorical_cols + text_cols + labels\n",
    "\n",
    "    df_relevant = df[relevant_cols]\n",
    "    \n",
    "    # Exploding required columns\n",
    "    df_relevant_explode_tweets = df_relevant.explode('tweet')\n",
    "\n",
    "    df_relevant = df_relevant_explode_tweets.copy()\n",
    "    \n",
    "    \"\"\"\n",
    "    Not using domain since it's not in elections\n",
    "\n",
    "    # Getting dummy variables for categorical\n",
    "    dummies = pd.get_dummies(df_relevant['domain'], drop_first = True)\n",
    "    \n",
    "    df_relevant = pd.concat([df_relevant, dummies], axis = 1)\n",
    "    \n",
    "    # dropping original categorical columns\n",
    "    df_relevant = df_relevant.drop(columns = ['domain'], axis = 1)\n",
    "    \"\"\"\n",
    "    # Getting correct data types\n",
    "    \n",
    "    # Numerical Data Types\n",
    "    df_relevant = df_relevant.copy()\n",
    "    df_relevant['profile.followers_count'] = df_relevant['profile.followers_count'].astype(float)\n",
    "    df_relevant['profile.friends_count'] = df_relevant['profile.friends_count'].astype(float)\n",
    "    df_relevant['profile.favourites_count'] = df_relevant['profile.favourites_count'].astype(float)\n",
    "    df_relevant['profile.listed_count'] = df_relevant['profile.listed_count'].astype(float)\n",
    "\n",
    "    # Categorical Types\n",
    "    df_relevant['profile.verified'] = df_relevant['profile.verified'].astype(str)\n",
    "\n",
    "    # Tweet\n",
    "    df_relevant['tweet'] = df_relevant['tweet'].astype(str)\n",
    "\n",
    "    # Labels\n",
    "    df_relevant['label'] = df_relevant['label'].astype(int)\n",
    "\n",
    "    # Encoding verified\n",
    "    df_relevant['profile.verified'] = df_relevant['profile.verified'].apply(encode_verified)\n",
    "    \n",
    "    # Tokenizing and Getting glove embeddings for tweets\n",
    "    tweets_df = df_relevant.copy()\n",
    "    \n",
    "    # Loading Glove Embeddings\n",
    "    glove_embeddings = load_glove_embeddings('../Data/glove.6B.200d.txt')\n",
    "\n",
    "    # Apply embedding to the 'Text' column\n",
    "    tweets_df['glove_emb'] = tweets_df['tweet'].apply(lambda x: embed_text(x, glove_embeddings))\n",
    "    \n",
    "    embedding_list = np.vstack(tweets_df['glove_emb'].values)\n",
    "\n",
    "    # EMBEDDINGS TENSOR\n",
    "    tweet_glove_embeddings = torch.tensor(embedding_list)\n",
    "    \n",
    "    # METADATA TENSOR\n",
    "    df_num_cat = df_relevant.copy()\n",
    "    df_num_cat = df_num_cat.drop(columns = ['tweet', 'label'])\n",
    "    \n",
    "    metadata_tensor = torch.tensor(df_num_cat.values)\n",
    "    \n",
    "    # LABEL TENSOR\n",
    "    label_tensor = torch.tensor(df_relevant['label'].values)\n",
    "    \n",
    "    return tweet_glove_embeddings, metadata_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce09a3",
   "metadata": {},
   "source": [
    "## Getting all tensors (test, train, validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd4b1bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:04:32.013879100Z",
     "start_time": "2025-03-29T18:03:10.551640600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199863, 200]) torch.Size([199863, 5]) torch.Size([199863])\n",
      "Finished Processing Test Data\n"
     ]
    }
   ],
   "source": [
    "test_tweet_emb, test_metadata_tensor, test_label = process_data('../Data/test.json')\n",
    "print(test_tweet_emb.shape, test_metadata_tensor.shape, test_label.shape)\n",
    "print(\"Finished Processing Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f68a4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:11:50.188327200Z",
     "start_time": "2025-03-29T18:04:32.018886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1398465, 200]) torch.Size([1398465, 5]) torch.Size([1398465])\n",
      "Finished Processing Train Data\n"
     ]
    }
   ],
   "source": [
    "train_tweet_emb, train_metadata_tensor, train_label = process_data('../Data/train.json')\n",
    "print(train_tweet_emb.shape, train_metadata_tensor.shape, train_label.shape)\n",
    "print(\"Finished Processing Train Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297f2575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:13:51.280674100Z",
     "start_time": "2025-03-29T18:11:50.183295800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([401540, 200]) torch.Size([401540, 5]) torch.Size([401540])\n",
      "Finished Processing Validate Data\n"
     ]
    }
   ],
   "source": [
    "validate_tweet_emb, validate_metadata_tensor, validate_label = process_data('../Data/dev.json')\n",
    "print(validate_tweet_emb.shape, validate_metadata_tensor.shape, validate_label.shape)\n",
    "print(\"Finished Processing Validate Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ../Data/Processed_Data\n"
     ]
    }
   ],
   "source": [
    "newpath = r\"../Data/Processed_Data\"\n",
    "\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath) \n",
    "    print(f\"Directory created: {newpath}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {newpath}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T20:47:25.202173Z",
     "start_time": "2025-03-29T20:47:25.197596300Z"
    }
   },
   "id": "234447f454c36771"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a3f03b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T18:21:27.491149700Z",
     "start_time": "2025-03-29T18:21:22.173512900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving tensor to files\n",
    "\n",
    "# Test\n",
    "torch.save(test_tweet_emb, '../Data/Processed_Data/test_tweet_emb_tensor.pth')\n",
    "torch.save(test_metadata_tensor, '../Data/Processed_Data/test_metadata_tensor.pth')\n",
    "torch.save(test_label, '../Data/Processed_Data/test_label_tensor.pth')\n",
    "\n",
    "# Train\n",
    "torch.save(train_tweet_emb, '../Data/Processed_Data/train_tweet_emb_tensor.pth')\n",
    "torch.save(train_metadata_tensor, '../Data/Processed_Data/train_metadata_tensor.pth')\n",
    "torch.save(train_label, '../Data/Processed_Data/train_label_tensor.pth')\n",
    "\n",
    "# Validate\n",
    "torch.save(validate_tweet_emb, '../Data/Processed_Data/validate_tweet_emb_tensor.pth')\n",
    "torch.save(validate_metadata_tensor, '../Data/Processed_Data/validate_metadata_tensor.pth')\n",
    "torch.save(validate_label, '../Data/Processed_Data/validate_label_tensor.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
