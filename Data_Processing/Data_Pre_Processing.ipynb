{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ce9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Reading Data\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11b337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/erickordonez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/erickordonez/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8134a2",
   "metadata": {},
   "source": [
    "# Pre Processing Data for each set (test, train, validate) and returning metadata tensor, tweet glove embeddings tensor, and labels tensor for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bddeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file_path):\n",
    "        embeddings = {}\n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "        return embeddings\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def embed_text(text, glove_embeddings):\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        embedding = []\n",
    "        \n",
    "        # Get GloVe embeddings for each word in the tokenized text\n",
    "        for word in tokens:\n",
    "            \n",
    "            # Applying Kudugunta's Rules\n",
    "            if word == '#':\n",
    "                \n",
    "                word = \"<hashtag>\"\n",
    "            \n",
    "            elif word == '@':\n",
    "                \n",
    "                word = \"<user>\"\n",
    "            \n",
    "            elif word == \"https\" or word == \"HTTPS\":\n",
    "                \n",
    "                word = \"<url>\"\n",
    "            \n",
    "            elif word[0:3] == \"//t\":\n",
    "                \n",
    "                word = \"<url>\"\n",
    "            \n",
    "            elif word.isdigit() or is_float(word):\n",
    "                \n",
    "                word = \"<number>\"\n",
    "                \n",
    "            # Replacing emojis\n",
    "            if \"üòÇ\" in word:\n",
    "                \n",
    "                word = word.replace(\"üòÇ\", \"<lolface>\")\n",
    "            \n",
    "            if \"‚ù§Ô∏è\" in word:\n",
    "                \n",
    "                word = word.replace(\"‚ù§Ô∏è\", \"<heart>\")\n",
    "                \n",
    "            if \"üòÅ\" in word:\n",
    "                \n",
    "                word = word.replace(\"üòÅ\", \"<smile>\")\n",
    "                \n",
    "            \n",
    "            # For word in all caps\n",
    "            if word.isupper():\n",
    "                \n",
    "                first_word = word.lower()\n",
    "                \n",
    "                if first_word in glove_embeddings:\n",
    "                    embedding.append(glove_embeddings[first_word])\n",
    "                else:\n",
    "                    # If the word is not in GloVe, append a zero vector (or you can handle it differently)\n",
    "                    embedding.append(np.zeros(200))  # Assuming the GloVe embeddings are 200-dimensional\n",
    "                    \n",
    "                second_word = \"<allcaps>\"\n",
    "                \n",
    "                if second_word in glove_embeddings:\n",
    "                    \n",
    "                    embedding.append(glove_embeddings[second_word])\n",
    "                else:\n",
    "                    # If the word is not in GloVe, append a zero vector (or you can handle it differently)\n",
    "                    embedding.append(np.zeros(200))  # Assuming the GloVe embeddings are 200-dimensional\n",
    "            \n",
    "            else: \n",
    "                word = word.lower()\n",
    "                \n",
    "                if word in glove_embeddings:\n",
    "                    embedding.append(glove_embeddings[word])\n",
    "                else:\n",
    "                    # If the word is not in GloVe, append a zero vector (or you can handle it differently)\n",
    "                    embedding.append(np.zeros(200))  # Assuming the GloVe embeddings are 200-dimensional\n",
    "\n",
    "        # Return the average embedding for the entire sentence (or you can return a list of vectors)\n",
    "        if embedding:\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(200)  # Default if no tokens are found in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c101693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_file_path):\n",
    "    \n",
    "    # Reading from json file\n",
    "    # Open and read the JSON file\n",
    "    with open(data_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Flatten the JSON using json_normalize\n",
    "    flattened_data = pd.json_normalize(data)\n",
    "\n",
    "    # Convert the flattened data into a Pandas DataFrame\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    \n",
    "    # Getting relevant columns\n",
    "    numerical_cols = ['profile.followers_count', 'profile.friends_count',\n",
    "                  'profile.favourites_count', 'profile.listed_count']\n",
    "\n",
    "    categorical_cols = ['profile.verified','domain']\n",
    "\n",
    "    text_cols = ['tweet']\n",
    "\n",
    "    labels = ['label']\n",
    "\n",
    "    relevant_cols = numerical_cols + categorical_cols + text_cols + labels\n",
    "\n",
    "    df_relevant = df[relevant_cols]\n",
    "    \n",
    "    # Exploding required columns\n",
    "    df_relevant_explode_tweets = df_relevant.explode('tweet')\n",
    "    df_relevant_explode_domain = df_relevant_explode_tweets.explode(\"domain\")\n",
    "\n",
    "    df_relevant = df_relevant_explode_domain.copy()\n",
    "    \n",
    "    # Getting dummy variables for categorical\n",
    "    dummies = pd.get_dummies(df_relevant['domain'], drop_first = True)\n",
    "    \n",
    "    df_relevant = pd.concat([df_relevant, dummies], axis = 1)\n",
    "    \n",
    "    # dropping original categorical columns\n",
    "    df_relevant = df_relevant.drop(columns = ['domain'], axis = 1)\n",
    "    \n",
    "    # Getting correct data types\n",
    "    \n",
    "    # Numerical Data Types\n",
    "    df_relevant = df_relevant.copy()\n",
    "    df_relevant['profile.followers_count'] = df_relevant['profile.followers_count'].astype(float)\n",
    "    df_relevant['profile.friends_count'] = df_relevant['profile.friends_count'].astype(float)\n",
    "    df_relevant['profile.favourites_count'] = df_relevant['profile.favourites_count'].astype(float)\n",
    "    df_relevant['profile.listed_count'] = df_relevant['profile.listed_count'].astype(float)\n",
    "\n",
    "    # Categorical Types\n",
    "    df_relevant['profile.verified'] = df_relevant['profile.verified'].astype(bool).astype(int)\n",
    "    df_relevant['Politics'] = df_relevant['Politics'].astype(bool).astype(int)\n",
    "    df_relevant['Entertainment'] = df_relevant['Entertainment'].astype(bool).astype(int)\n",
    "    df_relevant['Sports'] = df_relevant['Sports'].astype(bool).astype(int)\n",
    "\n",
    "    # Tweet\n",
    "    df_relevant['tweet'] = df_relevant['tweet'].astype(str)\n",
    "\n",
    "    # Labels\n",
    "    df_relevant['label'] = df_relevant['label'].astype(bool).astype(int)\n",
    "    \n",
    "    # Tokenizing and Getting glove embeddings for tweets\n",
    "    tweets_df = df_relevant.copy()\n",
    "    \n",
    "    # Loading Glove Embeddings\n",
    "    def load_glove_embeddings(glove_file_path):\n",
    "        embeddings = {}\n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "        return embeddings\n",
    "    glove_embeddings = load_glove_embeddings('glove.6B.200d.txt')\n",
    "    \n",
    "    def embed_text(text, glove_embeddings):\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        embedding = []\n",
    "\n",
    "        # Get GloVe embeddings for each word in the tokenized text\n",
    "        for word in tokens:\n",
    "            if word in glove_embeddings:\n",
    "                embedding.append(glove_embeddings[word])\n",
    "            else:\n",
    "                # If the word is not in GloVe, append a zero vector (or you can handle it differently)\n",
    "                embedding.append(np.zeros(100))  # Assuming the GloVe embeddings are 100-dimensional\n",
    "\n",
    "        # Return the average embedding for the entire sentence (or you can return a list of vectors)\n",
    "        if embedding:\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(100)  # Default if no tokens are found in the embeddings\n",
    "\n",
    "    # Apply embedding to the 'Text' column\n",
    "    tweets_df['glove_emb'] = tweets_df['tweet'].apply(lambda x: embed_text(x, glove_embeddings))\n",
    "    \n",
    "    embedding_list = np.vstack(tweets_df['glove_emb'].values)\n",
    "\n",
    "    # EMBEDDINGS TENSOR\n",
    "    tweet_glove_embeddings = torch.tensor(embedding_list)\n",
    "    \n",
    "    # METADATA TENSOR\n",
    "    df_num_cat = df_relevant.copy()\n",
    "    df_num_cat = df_num_cat.drop(columns = ['tweet', 'label'])\n",
    "    \n",
    "    metadata_tensor = torch.tensor(df_num_cat.values)\n",
    "    \n",
    "    # LABEL TENSOR\n",
    "    label_tensor = torch.tensor(df_relevant['label'].values)\n",
    "    \n",
    "    return tweet_glove_embeddings, metadata_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce09a3",
   "metadata": {},
   "source": [
    "# Getting all tensors (test, train, validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd4b1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([251066, 100]) torch.Size([251066, 8]) torch.Size([251066])\n",
      "Finished Processing Test Data\n"
     ]
    }
   ],
   "source": [
    "test_tweet_emb, test_metadata_tensor, test_label = process_data('test.json')\n",
    "print(test_tweet_emb.shape, test_metadata_tensor.shape, test_label.shape)\n",
    "print(\"Finished Processing Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f68a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1778865, 100]) torch.Size([1778865, 8]) torch.Size([1778865])\n",
      "Finished Processing Train Data\n"
     ]
    }
   ],
   "source": [
    "train_tweet_emb, train_metadata_tensor, train_label = process_data('train.json')\n",
    "print(train_tweet_emb.shape, train_metadata_tensor.shape, train_label.shape)\n",
    "print(\"Finished Processing Train Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "297f2575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512329, 100]) torch.Size([512329, 8]) torch.Size([512329])\n",
      "Finished Processing Validate Data\n"
     ]
    }
   ],
   "source": [
    "validate_tweet_emb, validate_metadata_tensor, validate_label = process_data('dev.json')\n",
    "print(validate_tweet_emb.shape, validate_metadata_tensor.shape, validate_label.shape)\n",
    "print(\"Finished Processing Validate Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1a3f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving tensor to files\n",
    "\n",
    "# Test\n",
    "torch.save(test_tweet_emb, 'test_tweet_emb_tensor.pth')\n",
    "torch.save(test_metadata_tensor, 'test_metadata_tensor.pth')\n",
    "torch.save(test_label, 'test_label_tensor.pth')\n",
    "\n",
    "# Train\n",
    "torch.save(train_tweet_emb, 'train_tweet_emb_tensor.pth')\n",
    "torch.save(train_metadata_tensor, 'train_metadata_tensor.pth')\n",
    "torch.save(train_label, 'train_label_tensor.pth')\n",
    "\n",
    "# Validate\n",
    "torch.save(validate_tweet_emb, 'validate_tweet_emb_tensor.pth')\n",
    "torch.save(validate_metadata_tensor, 'validate_metadata_tensor.pth')\n",
    "torch.save(validate_label, 'validate_label_tensor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92658175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
