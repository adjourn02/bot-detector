{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "CS 7643 Project\n",
    "\n",
    "Georgia Institute of Technology\n",
    "\n",
    "Author: Daniel Solon"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "908566b4b694ba17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Twibot-20 Dataset using RoBERTa embeddings\n",
    "Based on \"A Deep Learning Approach for Robust Detection of Bots in Twitter Using Transformers\" paper by Gutierrez et al. where their best model is based on (RoBERTa + metadata) for the input feature vectors fed to a Dense network."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97add8a6984aa61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ca9816439ea921"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5d9abab5ff54e4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Device\n",
    "Set device to CUDA if available, else CPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5e6e870e354e06d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48f434561beb0612"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Tweet Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc54967530e6068e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load RoBERTa tokenizer and model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5792e14ffa5644d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\").to(device)\n",
    "roberta_model.eval()\n",
    "\n",
    "def process_data(data_file_path, batch_size=64): \n",
    "    \"\"\"\n",
    "    :param data_file_path: json file path to be processed into embedding \n",
    "    :param batch_size: for tokenization; decrease if running into OOM errors\n",
    "    :return: RoBERTa embeddings using CLS token; length=768\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(data_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error loading JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Flatten JSON structure\n",
    "    flattened_data = pd.json_normalize(data)\n",
    "\n",
    "    # Extract tweets and ensure string type\n",
    "    if \"tweet\" not in flattened_data:\n",
    "        print(\"Error: 'tweet' column not found in data.\")\n",
    "        return None\n",
    "\n",
    "    df_relevant = flattened_data[[\"tweet\"]].explode(\"tweet\")\n",
    "    df_relevant[\"tweet\"] = df_relevant[\"tweet\"].astype(str)\n",
    "\n",
    "    # Batch tokenize tweets\n",
    "    tokenized_texts = tokenizer(\n",
    "        df_relevant[\"tweet\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128, # increase if tweets are longer than 128 tokens\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # Convert to tensors\n",
    "    input_ids = tokenized_texts[\"input_ids\"]\n",
    "    attention_mask = tokenized_texts[\"attention_mask\"]\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Processing Embeddings\"):\n",
    "            input_ids_batch, attention_mask_batch = batch\n",
    "            outputs = roberta_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)\n",
    "            embeddings.append(outputs.last_hidden_state[:, 0, :].detach().cpu())  # Use CLS token\n",
    "\n",
    "    # Stack embeddings into a single tensor\n",
    "    tweet_roberta_embeddings = torch.cat(embeddings)\n",
    "\n",
    "    return tweet_roberta_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e7d85d2a7a96672"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate embeddings for the datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b444ee090191766"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "test_tweet_emb = process_data(\"../Data/test.json\", batch_size=64)\n",
    "if test_tweet_emb is not None:\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
    "    print(test_tweet_emb.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9e3d8165f15328cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_tweet_emb = process_data('../Data/train.json', batch_size=64)\n",
    "if test_tweet_emb is not None:\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
    "    print(train_tweet_emb.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8231abab93b96bbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "validate_tweet_emb = process_data('../Data/dev.json', batch_size=64)\n",
    "if test_tweet_emb is not None:\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
    "    print(validate_tweet_emb.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78f83cb1d721e2e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Tweet Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5e538afff42c99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_data_path = r\"../Data/Processed_Data\"\n",
    "\n",
    "if not os.path.exists(processed_data_path):\n",
    "    os.makedirs(processed_data_path) \n",
    "    print(f\"Directory created: {processed_data_path}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {processed_data_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c129dabb188595b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test\n",
    "torch.save(test_tweet_emb, '../Data/Processed_Data/test_tweet_roberta_emb_tensor.pth')\n",
    "\n",
    "# Train\n",
    "torch.save(train_tweet_emb, '../Data/Processed_Data/train_tweet_roberta_emb_tensor.pth')\n",
    "\n",
    "# Validate\n",
    "torch.save(validate_tweet_emb, '../Data/Processed_Data/validate_tweet_roberta_emb_tensor.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53fb56341b8afb48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
